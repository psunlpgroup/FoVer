{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 100,
  "global_step": 625,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.016,
      "grad_norm": 535.8858313760783,
      "learning_rate": 3.194888178913738e-08,
      "loss": 3.2353,
      "step": 10
    },
    {
      "epoch": 0.032,
      "grad_norm": 448.7836862169865,
      "learning_rate": 6.389776357827476e-08,
      "loss": 2.9808,
      "step": 20
    },
    {
      "epoch": 0.048,
      "grad_norm": 420.8565871473787,
      "learning_rate": 9.584664536741213e-08,
      "loss": 2.77,
      "step": 30
    },
    {
      "epoch": 0.064,
      "grad_norm": 292.4213009742944,
      "learning_rate": 1.2779552715654952e-07,
      "loss": 2.3301,
      "step": 40
    },
    {
      "epoch": 0.08,
      "grad_norm": 116.36392586181292,
      "learning_rate": 1.5974440894568688e-07,
      "loss": 1.1065,
      "step": 50
    },
    {
      "epoch": 0.096,
      "grad_norm": 54.524511304458365,
      "learning_rate": 1.9169329073482426e-07,
      "loss": 0.4266,
      "step": 60
    },
    {
      "epoch": 0.112,
      "grad_norm": 27.161312142794664,
      "learning_rate": 2.2364217252396164e-07,
      "loss": 0.3471,
      "step": 70
    },
    {
      "epoch": 0.128,
      "grad_norm": 18.670977500257067,
      "learning_rate": 2.5559105431309904e-07,
      "loss": 0.2802,
      "step": 80
    },
    {
      "epoch": 0.144,
      "grad_norm": 15.885461901485572,
      "learning_rate": 2.875399361022364e-07,
      "loss": 0.295,
      "step": 90
    },
    {
      "epoch": 0.16,
      "grad_norm": 17.762046722732393,
      "learning_rate": 3.1948881789137375e-07,
      "loss": 0.2727,
      "step": 100
    },
    {
      "epoch": 0.16,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_loss": 0.07360175251960754,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_runtime": 4.8129,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_samples_per_second": 38.231,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_steps_per_second": 2.493,
      "step": 100
    },
    {
      "epoch": 0.176,
      "grad_norm": 24.710394878273785,
      "learning_rate": 3.514376996805112e-07,
      "loss": 0.2735,
      "step": 110
    },
    {
      "epoch": 0.192,
      "grad_norm": 24.919285860220974,
      "learning_rate": 3.833865814696485e-07,
      "loss": 0.2673,
      "step": 120
    },
    {
      "epoch": 0.208,
      "grad_norm": 17.102433271246618,
      "learning_rate": 4.1533546325878595e-07,
      "loss": 0.2207,
      "step": 130
    },
    {
      "epoch": 0.224,
      "grad_norm": 36.03873916944076,
      "learning_rate": 4.472843450479233e-07,
      "loss": 0.2423,
      "step": 140
    },
    {
      "epoch": 0.24,
      "grad_norm": 18.911534279344895,
      "learning_rate": 4.792332268370607e-07,
      "loss": 0.2201,
      "step": 150
    },
    {
      "epoch": 0.256,
      "grad_norm": 30.09066561473006,
      "learning_rate": 5.111821086261981e-07,
      "loss": 0.2075,
      "step": 160
    },
    {
      "epoch": 0.272,
      "grad_norm": 27.836367725698437,
      "learning_rate": 5.431309904153354e-07,
      "loss": 0.2288,
      "step": 170
    },
    {
      "epoch": 0.288,
      "grad_norm": 23.31485000593613,
      "learning_rate": 5.750798722044729e-07,
      "loss": 0.1705,
      "step": 180
    },
    {
      "epoch": 0.304,
      "grad_norm": 41.4310626737151,
      "learning_rate": 6.070287539936102e-07,
      "loss": 0.2111,
      "step": 190
    },
    {
      "epoch": 0.32,
      "grad_norm": 21.553450806788746,
      "learning_rate": 6.389776357827475e-07,
      "loss": 0.216,
      "step": 200
    },
    {
      "epoch": 0.32,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_loss": 0.06891260296106339,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_runtime": 4.2687,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_samples_per_second": 43.105,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_steps_per_second": 2.811,
      "step": 200
    },
    {
      "epoch": 0.336,
      "grad_norm": 24.225452007905094,
      "learning_rate": 6.70926517571885e-07,
      "loss": 0.2175,
      "step": 210
    },
    {
      "epoch": 0.352,
      "grad_norm": 38.6100363961472,
      "learning_rate": 7.028753993610224e-07,
      "loss": 0.2025,
      "step": 220
    },
    {
      "epoch": 0.368,
      "grad_norm": 25.747053949350214,
      "learning_rate": 7.348242811501597e-07,
      "loss": 0.1702,
      "step": 230
    },
    {
      "epoch": 0.384,
      "grad_norm": 22.70066364859232,
      "learning_rate": 7.66773162939297e-07,
      "loss": 0.1616,
      "step": 240
    },
    {
      "epoch": 0.4,
      "grad_norm": 23.231234212334883,
      "learning_rate": 7.987220447284346e-07,
      "loss": 0.193,
      "step": 250
    },
    {
      "epoch": 0.416,
      "grad_norm": 15.100503790283016,
      "learning_rate": 8.306709265175719e-07,
      "loss": 0.2268,
      "step": 260
    },
    {
      "epoch": 0.432,
      "grad_norm": 18.32935618983027,
      "learning_rate": 8.626198083067092e-07,
      "loss": 0.191,
      "step": 270
    },
    {
      "epoch": 0.448,
      "grad_norm": 13.697816344383048,
      "learning_rate": 8.945686900958466e-07,
      "loss": 0.184,
      "step": 280
    },
    {
      "epoch": 0.464,
      "grad_norm": 32.198600409849185,
      "learning_rate": 9.26517571884984e-07,
      "loss": 0.1742,
      "step": 290
    },
    {
      "epoch": 0.48,
      "grad_norm": 10.509781735897775,
      "learning_rate": 9.584664536741213e-07,
      "loss": 0.1803,
      "step": 300
    },
    {
      "epoch": 0.48,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_loss": 0.050013795495033264,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_runtime": 4.551,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_samples_per_second": 40.431,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_steps_per_second": 2.637,
      "step": 300
    },
    {
      "epoch": 0.496,
      "grad_norm": 33.600403908547776,
      "learning_rate": 9.904153354632587e-07,
      "loss": 0.1643,
      "step": 310
    },
    {
      "epoch": 0.512,
      "grad_norm": 19.353229396088178,
      "learning_rate": 9.775641025641025e-07,
      "loss": 0.1488,
      "step": 320
    },
    {
      "epoch": 0.528,
      "grad_norm": 22.327070755853306,
      "learning_rate": 9.455128205128204e-07,
      "loss": 0.1673,
      "step": 330
    },
    {
      "epoch": 0.544,
      "grad_norm": 22.489228189629124,
      "learning_rate": 9.134615384615383e-07,
      "loss": 0.1471,
      "step": 340
    },
    {
      "epoch": 0.56,
      "grad_norm": 17.27665743294894,
      "learning_rate": 8.814102564102564e-07,
      "loss": 0.1737,
      "step": 350
    },
    {
      "epoch": 0.576,
      "grad_norm": 8.999714528441281,
      "learning_rate": 8.493589743589743e-07,
      "loss": 0.1261,
      "step": 360
    },
    {
      "epoch": 0.592,
      "grad_norm": 32.128803527659706,
      "learning_rate": 8.173076923076923e-07,
      "loss": 0.1284,
      "step": 370
    },
    {
      "epoch": 0.608,
      "grad_norm": 20.791536131597436,
      "learning_rate": 7.852564102564102e-07,
      "loss": 0.1485,
      "step": 380
    },
    {
      "epoch": 0.624,
      "grad_norm": 22.118551365567424,
      "learning_rate": 7.532051282051282e-07,
      "loss": 0.1501,
      "step": 390
    },
    {
      "epoch": 0.64,
      "grad_norm": 29.34475053958418,
      "learning_rate": 7.211538461538461e-07,
      "loss": 0.1538,
      "step": 400
    },
    {
      "epoch": 0.64,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_loss": 0.025828808546066284,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_runtime": 4.779,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_samples_per_second": 38.502,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_steps_per_second": 2.511,
      "step": 400
    },
    {
      "epoch": 0.656,
      "grad_norm": 27.86599315905633,
      "learning_rate": 6.89102564102564e-07,
      "loss": 0.1465,
      "step": 410
    },
    {
      "epoch": 0.672,
      "grad_norm": 16.16599823210772,
      "learning_rate": 6.57051282051282e-07,
      "loss": 0.1512,
      "step": 420
    },
    {
      "epoch": 0.688,
      "grad_norm": 30.018947250115986,
      "learning_rate": 6.249999999999999e-07,
      "loss": 0.1491,
      "step": 430
    },
    {
      "epoch": 0.704,
      "grad_norm": 12.003406080617301,
      "learning_rate": 5.92948717948718e-07,
      "loss": 0.1342,
      "step": 440
    },
    {
      "epoch": 0.72,
      "grad_norm": 16.084906330744676,
      "learning_rate": 5.608974358974359e-07,
      "loss": 0.1309,
      "step": 450
    },
    {
      "epoch": 0.736,
      "grad_norm": 14.21636517225008,
      "learning_rate": 5.288461538461539e-07,
      "loss": 0.1445,
      "step": 460
    },
    {
      "epoch": 0.752,
      "grad_norm": 24.33573398328395,
      "learning_rate": 4.967948717948718e-07,
      "loss": 0.1506,
      "step": 470
    },
    {
      "epoch": 0.768,
      "grad_norm": 16.857099879581632,
      "learning_rate": 4.6474358974358975e-07,
      "loss": 0.1586,
      "step": 480
    },
    {
      "epoch": 0.784,
      "grad_norm": 13.204804708822099,
      "learning_rate": 4.326923076923077e-07,
      "loss": 0.1329,
      "step": 490
    },
    {
      "epoch": 0.8,
      "grad_norm": 24.988751795539695,
      "learning_rate": 4.006410256410256e-07,
      "loss": 0.1351,
      "step": 500
    },
    {
      "epoch": 0.8,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_loss": 0.04164096340537071,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_runtime": 4.1002,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_samples_per_second": 44.876,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_steps_per_second": 2.927,
      "step": 500
    },
    {
      "epoch": 0.816,
      "grad_norm": 15.643103182850238,
      "learning_rate": 3.685897435897436e-07,
      "loss": 0.1336,
      "step": 510
    },
    {
      "epoch": 0.832,
      "grad_norm": 13.542878186933619,
      "learning_rate": 3.3653846153846154e-07,
      "loss": 0.1311,
      "step": 520
    },
    {
      "epoch": 0.848,
      "grad_norm": 15.116806647451414,
      "learning_rate": 3.0448717948717945e-07,
      "loss": 0.1111,
      "step": 530
    },
    {
      "epoch": 0.864,
      "grad_norm": 18.18824075068164,
      "learning_rate": 2.724358974358974e-07,
      "loss": 0.0987,
      "step": 540
    },
    {
      "epoch": 0.88,
      "grad_norm": 12.379759228319903,
      "learning_rate": 2.4038461538461537e-07,
      "loss": 0.1006,
      "step": 550
    },
    {
      "epoch": 0.896,
      "grad_norm": 12.654567718515528,
      "learning_rate": 2.0833333333333333e-07,
      "loss": 0.1188,
      "step": 560
    },
    {
      "epoch": 0.912,
      "grad_norm": 7.646197123708283,
      "learning_rate": 1.762820512820513e-07,
      "loss": 0.0792,
      "step": 570
    },
    {
      "epoch": 0.928,
      "grad_norm": 10.943103675905594,
      "learning_rate": 1.442307692307692e-07,
      "loss": 0.0974,
      "step": 580
    },
    {
      "epoch": 0.944,
      "grad_norm": 26.614553350280946,
      "learning_rate": 1.1217948717948718e-07,
      "loss": 0.0887,
      "step": 590
    },
    {
      "epoch": 0.96,
      "grad_norm": 23.363537073342762,
      "learning_rate": 8.012820512820514e-08,
      "loss": 0.1206,
      "step": 600
    },
    {
      "epoch": 0.96,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_loss": 0.03844239190220833,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_runtime": 4.3434,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_samples_per_second": 42.363,
      "eval_FoVer_PRM_FormalProof_10k_Llama-3.1-8B-Instruct_validation_steps_per_second": 2.763,
      "step": 600
    },
    {
      "epoch": 0.976,
      "grad_norm": 31.36653808122187,
      "learning_rate": 4.807692307692308e-08,
      "loss": 0.1544,
      "step": 610
    },
    {
      "epoch": 0.992,
      "grad_norm": 25.642473919090573,
      "learning_rate": 1.6025641025641023e-08,
      "loss": 0.1331,
      "step": 620
    },
    {
      "epoch": 1.0,
      "step": 625,
      "total_flos": 15601289134080.0,
      "train_loss": 0.36171485652923585,
      "train_runtime": 1815.3156,
      "train_samples_per_second": 11.017,
      "train_steps_per_second": 0.344
    }
  ],
  "logging_steps": 10,
  "max_steps": 625,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 1000000000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 15601289134080.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
