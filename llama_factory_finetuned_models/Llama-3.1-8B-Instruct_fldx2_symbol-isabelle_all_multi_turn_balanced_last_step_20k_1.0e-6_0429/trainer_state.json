{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 100,
  "global_step": 625,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.016,
      "grad_norm": 457.0006473982902,
      "learning_rate": 3.194888178913738e-08,
      "loss": 3.5124,
      "step": 10
    },
    {
      "epoch": 0.032,
      "grad_norm": 448.17898501534074,
      "learning_rate": 6.389776357827476e-08,
      "loss": 3.5009,
      "step": 20
    },
    {
      "epoch": 0.048,
      "grad_norm": 498.79824728802737,
      "learning_rate": 9.584664536741213e-08,
      "loss": 2.9738,
      "step": 30
    },
    {
      "epoch": 0.064,
      "grad_norm": 289.91637561577994,
      "learning_rate": 1.2779552715654952e-07,
      "loss": 2.5673,
      "step": 40
    },
    {
      "epoch": 0.08,
      "grad_norm": 184.34187410400594,
      "learning_rate": 1.5974440894568688e-07,
      "loss": 1.5706,
      "step": 50
    },
    {
      "epoch": 0.096,
      "grad_norm": 33.573951956823294,
      "learning_rate": 1.9169329073482426e-07,
      "loss": 0.409,
      "step": 60
    },
    {
      "epoch": 0.112,
      "grad_norm": 26.58860422837688,
      "learning_rate": 2.2364217252396164e-07,
      "loss": 0.3612,
      "step": 70
    },
    {
      "epoch": 0.128,
      "grad_norm": 50.94735273880274,
      "learning_rate": 2.5559105431309904e-07,
      "loss": 0.3136,
      "step": 80
    },
    {
      "epoch": 0.144,
      "grad_norm": 42.2312235334911,
      "learning_rate": 2.875399361022364e-07,
      "loss": 0.304,
      "step": 90
    },
    {
      "epoch": 0.16,
      "grad_norm": 32.183605775389694,
      "learning_rate": 3.1948881789137375e-07,
      "loss": 0.3018,
      "step": 100
    },
    {
      "epoch": 0.16,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_loss": 0.2834005057811737,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_runtime": 6.3321,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_samples_per_second": 49.747,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_steps_per_second": 3.159,
      "step": 100
    },
    {
      "epoch": 0.176,
      "grad_norm": 22.3878696751142,
      "learning_rate": 3.514376996805112e-07,
      "loss": 0.2815,
      "step": 110
    },
    {
      "epoch": 0.192,
      "grad_norm": 27.76973091085007,
      "learning_rate": 3.833865814696485e-07,
      "loss": 0.2741,
      "step": 120
    },
    {
      "epoch": 0.208,
      "grad_norm": 19.303030331680105,
      "learning_rate": 4.1533546325878595e-07,
      "loss": 0.2788,
      "step": 130
    },
    {
      "epoch": 0.224,
      "grad_norm": 151.29661134782646,
      "learning_rate": 4.472843450479233e-07,
      "loss": 0.295,
      "step": 140
    },
    {
      "epoch": 0.24,
      "grad_norm": 26.218653193417417,
      "learning_rate": 4.792332268370607e-07,
      "loss": 0.2794,
      "step": 150
    },
    {
      "epoch": 0.256,
      "grad_norm": 32.452081581161465,
      "learning_rate": 5.111821086261981e-07,
      "loss": 0.2419,
      "step": 160
    },
    {
      "epoch": 0.272,
      "grad_norm": 34.900474563746755,
      "learning_rate": 5.431309904153354e-07,
      "loss": 0.2413,
      "step": 170
    },
    {
      "epoch": 0.288,
      "grad_norm": 28.494609743392296,
      "learning_rate": 5.750798722044729e-07,
      "loss": 0.2523,
      "step": 180
    },
    {
      "epoch": 0.304,
      "grad_norm": 29.273378705788048,
      "learning_rate": 6.070287539936102e-07,
      "loss": 0.203,
      "step": 190
    },
    {
      "epoch": 0.32,
      "grad_norm": 22.495678857586324,
      "learning_rate": 6.389776357827475e-07,
      "loss": 0.2641,
      "step": 200
    },
    {
      "epoch": 0.32,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_loss": 0.23759333789348602,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_runtime": 6.2425,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_samples_per_second": 50.461,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_steps_per_second": 3.204,
      "step": 200
    },
    {
      "epoch": 0.336,
      "grad_norm": 17.44171346986909,
      "learning_rate": 6.70926517571885e-07,
      "loss": 0.2201,
      "step": 210
    },
    {
      "epoch": 0.352,
      "grad_norm": 33.30873148417707,
      "learning_rate": 7.028753993610224e-07,
      "loss": 0.2239,
      "step": 220
    },
    {
      "epoch": 0.368,
      "grad_norm": 45.915685800911476,
      "learning_rate": 7.348242811501597e-07,
      "loss": 0.196,
      "step": 230
    },
    {
      "epoch": 0.384,
      "grad_norm": 33.11983561340508,
      "learning_rate": 7.66773162939297e-07,
      "loss": 0.1874,
      "step": 240
    },
    {
      "epoch": 0.4,
      "grad_norm": 30.263109095504472,
      "learning_rate": 7.987220447284346e-07,
      "loss": 0.172,
      "step": 250
    },
    {
      "epoch": 0.416,
      "grad_norm": 21.746138482853496,
      "learning_rate": 8.306709265175719e-07,
      "loss": 0.2317,
      "step": 260
    },
    {
      "epoch": 0.432,
      "grad_norm": 33.72310615066145,
      "learning_rate": 8.626198083067092e-07,
      "loss": 0.1802,
      "step": 270
    },
    {
      "epoch": 0.448,
      "grad_norm": 22.53533016229811,
      "learning_rate": 8.945686900958466e-07,
      "loss": 0.192,
      "step": 280
    },
    {
      "epoch": 0.464,
      "grad_norm": 35.93918447393091,
      "learning_rate": 9.26517571884984e-07,
      "loss": 0.1737,
      "step": 290
    },
    {
      "epoch": 0.48,
      "grad_norm": 12.356798841179385,
      "learning_rate": 9.584664536741213e-07,
      "loss": 0.1832,
      "step": 300
    },
    {
      "epoch": 0.48,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_loss": 0.22728750109672546,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_runtime": 6.2105,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_samples_per_second": 50.721,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_steps_per_second": 3.22,
      "step": 300
    },
    {
      "epoch": 0.496,
      "grad_norm": 18.886301025781812,
      "learning_rate": 9.904153354632587e-07,
      "loss": 0.1408,
      "step": 310
    },
    {
      "epoch": 0.512,
      "grad_norm": 36.8940515603227,
      "learning_rate": 9.775641025641025e-07,
      "loss": 0.1786,
      "step": 320
    },
    {
      "epoch": 0.528,
      "grad_norm": 9.018629153104385,
      "learning_rate": 9.455128205128204e-07,
      "loss": 0.1664,
      "step": 330
    },
    {
      "epoch": 0.544,
      "grad_norm": 37.40227783700545,
      "learning_rate": 9.134615384615383e-07,
      "loss": 0.1831,
      "step": 340
    },
    {
      "epoch": 0.56,
      "grad_norm": 16.70151353409806,
      "learning_rate": 8.814102564102564e-07,
      "loss": 0.1962,
      "step": 350
    },
    {
      "epoch": 0.576,
      "grad_norm": 15.143538312052346,
      "learning_rate": 8.493589743589743e-07,
      "loss": 0.1783,
      "step": 360
    },
    {
      "epoch": 0.592,
      "grad_norm": 35.15757764701661,
      "learning_rate": 8.173076923076923e-07,
      "loss": 0.1508,
      "step": 370
    },
    {
      "epoch": 0.608,
      "grad_norm": 33.34189165691871,
      "learning_rate": 7.852564102564102e-07,
      "loss": 0.174,
      "step": 380
    },
    {
      "epoch": 0.624,
      "grad_norm": 37.41883059316534,
      "learning_rate": 7.532051282051282e-07,
      "loss": 0.1482,
      "step": 390
    },
    {
      "epoch": 0.64,
      "grad_norm": 25.84453785889263,
      "learning_rate": 7.211538461538461e-07,
      "loss": 0.1487,
      "step": 400
    },
    {
      "epoch": 0.64,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_loss": 0.22872355580329895,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_runtime": 6.2768,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_samples_per_second": 50.185,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_steps_per_second": 3.186,
      "step": 400
    },
    {
      "epoch": 0.656,
      "grad_norm": 19.78638513121553,
      "learning_rate": 6.89102564102564e-07,
      "loss": 0.1498,
      "step": 410
    },
    {
      "epoch": 0.672,
      "grad_norm": 34.975188946169304,
      "learning_rate": 6.57051282051282e-07,
      "loss": 0.1551,
      "step": 420
    },
    {
      "epoch": 0.688,
      "grad_norm": 16.0252499449557,
      "learning_rate": 6.249999999999999e-07,
      "loss": 0.1628,
      "step": 430
    },
    {
      "epoch": 0.704,
      "grad_norm": 17.51308738108786,
      "learning_rate": 5.92948717948718e-07,
      "loss": 0.1564,
      "step": 440
    },
    {
      "epoch": 0.72,
      "grad_norm": 20.89330772619463,
      "learning_rate": 5.608974358974359e-07,
      "loss": 0.1785,
      "step": 450
    },
    {
      "epoch": 0.736,
      "grad_norm": 7.007950536172175,
      "learning_rate": 5.288461538461539e-07,
      "loss": 0.154,
      "step": 460
    },
    {
      "epoch": 0.752,
      "grad_norm": 9.964057094776031,
      "learning_rate": 4.967948717948718e-07,
      "loss": 0.1356,
      "step": 470
    },
    {
      "epoch": 0.768,
      "grad_norm": 14.667307120966946,
      "learning_rate": 4.6474358974358975e-07,
      "loss": 0.1205,
      "step": 480
    },
    {
      "epoch": 0.784,
      "grad_norm": 27.588944134566116,
      "learning_rate": 4.326923076923077e-07,
      "loss": 0.1431,
      "step": 490
    },
    {
      "epoch": 0.8,
      "grad_norm": 12.408541799795856,
      "learning_rate": 4.006410256410256e-07,
      "loss": 0.136,
      "step": 500
    },
    {
      "epoch": 0.8,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_loss": 0.23764581978321075,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_runtime": 6.2236,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_samples_per_second": 50.614,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_steps_per_second": 3.214,
      "step": 500
    },
    {
      "epoch": 0.816,
      "grad_norm": 12.663178116681529,
      "learning_rate": 3.685897435897436e-07,
      "loss": 0.1262,
      "step": 510
    },
    {
      "epoch": 0.832,
      "grad_norm": 12.853923248248984,
      "learning_rate": 3.3653846153846154e-07,
      "loss": 0.14,
      "step": 520
    },
    {
      "epoch": 0.848,
      "grad_norm": 20.78241324767642,
      "learning_rate": 3.0448717948717945e-07,
      "loss": 0.1388,
      "step": 530
    },
    {
      "epoch": 0.864,
      "grad_norm": 21.13552069490175,
      "learning_rate": 2.724358974358974e-07,
      "loss": 0.1619,
      "step": 540
    },
    {
      "epoch": 0.88,
      "grad_norm": 17.390014598100414,
      "learning_rate": 2.4038461538461537e-07,
      "loss": 0.1423,
      "step": 550
    },
    {
      "epoch": 0.896,
      "grad_norm": 52.45733118114345,
      "learning_rate": 2.0833333333333333e-07,
      "loss": 0.125,
      "step": 560
    },
    {
      "epoch": 0.912,
      "grad_norm": 13.522622898663343,
      "learning_rate": 1.762820512820513e-07,
      "loss": 0.1043,
      "step": 570
    },
    {
      "epoch": 0.928,
      "grad_norm": 10.012506046203828,
      "learning_rate": 1.442307692307692e-07,
      "loss": 0.1101,
      "step": 580
    },
    {
      "epoch": 0.944,
      "grad_norm": 25.060538841734605,
      "learning_rate": 1.1217948717948718e-07,
      "loss": 0.1481,
      "step": 590
    },
    {
      "epoch": 0.96,
      "grad_norm": 18.67905551628987,
      "learning_rate": 8.012820512820514e-08,
      "loss": 0.1383,
      "step": 600
    },
    {
      "epoch": 0.96,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_loss": 0.24290978908538818,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_runtime": 6.2214,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_samples_per_second": 50.632,
      "eval_FoVer_PRM_FormalLogic-FormalProof_10k_Llama-3.1-8B-Instruct_validation_steps_per_second": 3.215,
      "step": 600
    },
    {
      "epoch": 0.976,
      "grad_norm": 20.54982190423151,
      "learning_rate": 4.807692307692308e-08,
      "loss": 0.1263,
      "step": 610
    },
    {
      "epoch": 0.992,
      "grad_norm": 8.005756681958836,
      "learning_rate": 1.6025641025641023e-08,
      "loss": 0.1486,
      "step": 620
    },
    {
      "epoch": 1.0,
      "step": 625,
      "total_flos": 18834354339840.0,
      "train_loss": 0.40366163539886474,
      "train_runtime": 1653.8031,
      "train_samples_per_second": 12.093,
      "train_steps_per_second": 0.378
    }
  ],
  "logging_steps": 10,
  "max_steps": 625,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 1000000000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 18834354339840.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
